{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('models/cifar')\n",
    "\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "# from torchvision import models\n",
    "import numpy as np \n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import copy\n",
    "from utils import AverageMeter, accuracy\n",
    "import models.cifar as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluates the test accuracy of the model. This is used to evaluate the importance of a filter after removing it from the model.\n",
    "This code is taken directly from the training code that plots the test error.\n",
    "'''\n",
    "def test(testloader, model, criterion, use_cuda):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "        \n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        # plot progress\n",
    "    print(\"top1.avg\", top1.avg)\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__loader__', 'resnet', '__package__', 'alexnet_n2', 'densenet', 'scatfirst4', 'alexnet', 'scatfirst2', 'VGG', 'preresnet', 'vgg13', 'vgg13_bn', '__spec__', '__path__', '__name__', 'vgg16', 'scatfirst3', 'scatfirst', 'absolute_import', 'resnext', 'alexscat', 'alexscat_fnum', 'wrn', '__file__', '__builtins__', '__cached__', 'alexscat_fnum_n2', 'scatfirst_fnum', 'vgg11', 'vgg16_bn', 'vgg19', 'vgg11_bn', 'alexscat2first', 'scatonly', 'vgg19_bn'])\n"
     ]
    }
   ],
   "source": [
    "def get_model_weights(model_checkpoint=\"model_best_j2l2_n2.tar\"):\n",
    "    \n",
    "    #This code loads the model in and extracts the conv1_weights\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "    best_ascat = torch.load(model_checkpoint)\n",
    "    print(models.__dict__.keys())\n",
    "#     model = models.__dict__[\"alexscat_fnum_n2\"](num_classes=100,n=32,j=2,l=2)\n",
    "    model = models.__dict__[\"alexnet\"](num_classes=100)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(best_ascat['state_dict'])\n",
    "#     conv1_weights = best_ascat['state_dict']['module.first_layer.0.weight'].cpu().numpy()\n",
    "    conv1_weights = best_ascat['state_dict']['module.features.0.weight'].cpu().numpy()\n",
    "    model_weights = np.swapaxes(conv1_weights, 1, 3)\n",
    "    return model, model_weights, best_ascat\n",
    "\n",
    "model, model_weights, best_ascat = get_model_weights(\"alexnet_best.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1.avg\n",
      "44.53\n",
      "TEST OVER\n"
     ]
    }
   ],
   "source": [
    "#This code prepares the data to evaluate the test accuracy. This was taken directly from the training code.\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "dataloader = datasets.CIFAR100\n",
    "testset = dataloader(root='/scratch/users/vision/data/cifar100', train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#This is another copy of the same model so we can alter it.\n",
    "model2 = models.__dict__[\"alexnet\"](num_classes=100)\n",
    "# model2 = models.__dict__[\"alexscat_fnum_n2\"](num_classes=100,n=32,j=2,l=2)\n",
    "model2 = torch.nn.DataParallel(model2).cuda()\n",
    "best_ascat2 = copy.deepcopy(best_ascat['state_dict'])   \n",
    "model2.load_state_dict(best_ascat2)\n",
    "\n",
    "losses, allFilters = test(testloader, model2, criterion, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1.avg 42.51\n",
      "top1.avg 43.99\n",
      "top1.avg 42.57\n",
      "top1.avg 44.03\n",
      "top1.avg 43.61\n",
      "top1.avg 43.4\n",
      "top1.avg 43.45\n",
      "top1.avg 43.86\n",
      "top1.avg 43.03\n",
      "top1.avg 42.89\n",
      "top1.avg 43.43\n",
      "top1.avg 44.4\n",
      "top1.avg 43.07\n",
      "top1.avg 43.9\n"
     ]
    }
   ],
   "source": [
    "# This code just gets the importance scores of each filter\n",
    "def calc_scores(best_ascat2, model2):\n",
    "    scores = []\n",
    "    for f_num in range(14):\n",
    "    # for f_num in range(model.module.n_flayer - model.module.nfscat*3):\n",
    "        best_ascat2 = copy.deepcopy(best_ascat['state_dict'])\n",
    "        best_ascat2['module.features.0.weight'][f_num] = 0\n",
    "        model2.load_state_dict(best_ascat2)\n",
    "        losses, top1 = test(testloader, model2, criterion, True)\n",
    "        scores.append(top1)\n",
    "    scores = np.array(scores)\n",
    "    return scores\n",
    "scores = calc_scores(best_ascat2, model2)\n",
    "\n",
    "\n",
    "#if not os.path.exists('../importance/j2l3'):\n",
    "#    os.makedirs('../importance/j2l3')\n",
    "\n",
    "def plot_filters(scores, model_weights):\n",
    "    #This code goes through and plots each filter.\n",
    "    num_cols = 8\n",
    "    num_rows = 1 + len(scores)//num_cols\n",
    "    fig = plt.figure(figsize=(num_cols, num_rows))\n",
    "    for importance, f_num in enumerate(np.argsort(scores)):\n",
    "        ax1 = fig.add_subplot(num_rows, num_cols, importance + 1)\n",
    "        minned = model_weights[f_num] - np.min(model_weights[f_num])\n",
    "        ax1.imshow(minned/np.max(minned))\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_title('{0:.2f}'.format(allFilters - scores[f_num]))\n",
    "    plt.subplots_adjust(wspace=1.0, hspace=0.1)\n",
    "    plt.savefig(\"TEST_SCHANNEL.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter max activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-49fa8cf0adbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#This code applies the maximal activation for each filter and then plots the resulting image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mREGULARIZATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Makes a noisy image and turns it into a torch array so we can later turn it into a variable and calculate gradients on it and apply them.\n",
    "'''\n",
    "def make_image(): \n",
    "    images = []\n",
    "    im_dim = 11\n",
    "    for _ in range(3):\n",
    "        image = np.random.randn(im_dim, im_dim)/5+0.5\n",
    "        image = np.pad(image, ((im_dim, im_dim), (im_dim, im_dim)), 'constant')\n",
    "        images.append(image)\n",
    "    image = np.stack(images)\n",
    "    image = image.transpose(0, 2, 1)\n",
    "    \n",
    "    torch_image = torch.from_numpy(image).float()\n",
    "    torch_image = torch_image.unsqueeze_(0)\n",
    "    return torch_image\n",
    "\n",
    "#This code applies the maximal activation for each filter and then plots the resulting image.\n",
    "\n",
    "fig = plt.figure(figsize=(num_cols, num_rows))\n",
    "REGULARIZATION = 0.0001\n",
    "for idx, f_num in enumerate(np.argsort(scores)):\n",
    "#     print(idx)\n",
    "    torch_image = make_image()\n",
    "#     print(torch_image.shape)\n",
    "    im_as_var = Variable(torch_image.cuda(), requires_grad=True)\n",
    "    optimizer = SGD([im_as_var], lr=12,  weight_decay=1e-4)\n",
    "    for i in range(1, 501):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         x = im_as_var\n",
    "        y = model.module.features[0](im_as_var)\n",
    "#         print(y.shape) ([1, 64, 9, 9])\n",
    "        #x = x.view(x.size(0), model.module.nfscat*3, model.module.nspace, model.module.nspace)\n",
    "        loss = -1 * y[0, f_num, 4, 4]\n",
    "#         loss = -1 * torch.sum(y)\n",
    "\n",
    "#         reg_loss = REGULARIZATION * (\n",
    "#             torch.sum(torch.abs(im_as_var[:, :, :-1] - im_as_var[ :, :, 1:])) + \n",
    "#             torch.sum(torch.abs(im_as_var[ :, :-1, :] - im_as_var[:, 1:, :]))\n",
    "#         )\n",
    "        reg_loss = 0\n",
    "\n",
    "\n",
    "        loss = loss + reg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    recreated_im = copy.copy(im_as_var.data.cpu().numpy()[0]).transpose(2, 1, 0)\n",
    "    recreated_im = recreated_im[11:22, 11:22, :]\n",
    "    minned = recreated_im - np.min(recreated_im)\n",
    "    ax1 = fig.add_subplot(num_rows, num_cols, idx + 1)\n",
    "    ax1.imshow(minned/np.max(minned))\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_title('{0:.2f}'.format(allFilters - scores[f_num]))\n",
    "\n",
    "plt.subplots_adjust(wspace=1.0, hspace=0.1)\n",
    "plt.savefig(\"TEST_SCHANNEL.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_filters(scores, model_weights)\n",
    "plt.show()\n",
    "\n",
    "\t#cv2.imwrite(\"../importance/j2l3/\"+\"i\"+str(importance)+\"a\"+str(scores[f_num])+\"f\"+str(f_num)+\"dream.jpg\", fin*255)\n",
    "\t#minned = model_weights[f_num] - np.min(model_weights[f_num])\n",
    "\t#fin = minned/np.max(minned)\n",
    "\t#cv2.imwrite(\"../importance/j2l3/\"+\"i\"+str(importance)+\"a\"+str(scores[f_num])+\"f\"+str(f_num)+\"filter.jpg\", fin*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
